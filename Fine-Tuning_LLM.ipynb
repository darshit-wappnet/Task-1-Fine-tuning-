{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q\n!pip install peft ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:55:16.231390Z","iopub.execute_input":"2025-06-02T08:55:16.231702Z","iopub.status.idle":"2025-06-02T08:56:43.762504Z","shell.execute_reply.started":"2025-06-02T08:55:16.231678Z","shell.execute_reply":"2025-06-02T08:56:43.761476Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.3/141.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.9/412.9 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.51.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.31.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2025.3.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.4.26)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset,DatasetDict\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom transformers.trainer_utils import get_last_checkpoint \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:56:43.763565Z","iopub.execute_input":"2025-06-02T08:56:43.763789Z","iopub.status.idle":"2025-06-02T08:57:13.223378Z","shell.execute_reply.started":"2025-06-02T08:56:43.763762Z","shell.execute_reply":"2025-06-02T08:57:13.222500Z"}},"outputs":[{"name":"stderr","text":"2025-06-02 08:57:00.046315: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748854620.249917      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748854620.311221      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset, concatenate_datasets, DatasetDict\n\ndataset = load_dataset(\"SurAyush/News_Summary_Dataset\")[\"train\"]\ndata_1 = load_dataset(\"gopalkalpande/bbc-news-summary\")[\"train\"]\n\ndata_1 = data_1.remove_columns([\"File_path\"])\ndata_1 = data_1.rename_columns({\"Articles\": \"article\", \"Summaries\": \"summary\"})\n\ncombined_dataset = concatenate_datasets([data_1, dataset])\n\nfinal_dataset = DatasetDict({\"train\": combined_dataset})\n\n# Preview\nprint(final_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:57:13.225242Z","iopub.execute_input":"2025-06-02T08:57:13.225749Z","iopub.status.idle":"2025-06-02T08:57:15.739463Z","shell.execute_reply.started":"2025-06-02T08:57:13.225729Z","shell.execute_reply":"2025-06-02T08:57:15.738896Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.22k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46971c4960704bcaa9e733bc23aff063"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"news_summaries.csv:   0%|          | 0.00/7.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32d4f22c18924047a6d130bc928b6162"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2224 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e40750c7b8874e7fabdd9f6b8309e82b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca8830e2c8344b7b9d225c55340d97a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"bbc-news-summary.csv:   0%|          | 0.00/7.32M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"538a91f0dae84b4fa32f31ea24e3577f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2224 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62f20ad444134c70ad2e95f1ef8b14ea"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['article', 'summary'],\n        num_rows: 4448\n    })\n})\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"split_dataset =  final_dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\nprint(\"Dataset after split:\", split_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:57:15.740086Z","iopub.execute_input":"2025-06-02T08:57:15.740358Z","iopub.status.idle":"2025-06-02T08:57:15.753193Z","shell.execute_reply.started":"2025-06-02T08:57:15.740332Z","shell.execute_reply":"2025-06-02T08:57:15.752389Z"}},"outputs":[{"name":"stdout","text":"Dataset after split: DatasetDict({\n    train: Dataset({\n        features: ['article', 'summary'],\n        num_rows: 4003\n    })\n    test: Dataset({\n        features: ['article', 'summary'],\n        num_rows: 445\n    })\n})\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"model_name = \"t5-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:57:15.754080Z","iopub.execute_input":"2025-06-02T08:57:15.754267Z","iopub.status.idle":"2025-06-02T08:57:20.551781Z","shell.execute_reply.started":"2025-06-02T08:57:15.754253Z","shell.execute_reply":"2025-06-02T08:57:20.550789Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3dd9b41fe194a62b1a9544e79c4705f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c16d0fbf71b24e71ab15948452c141fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a65bec0e657477688bc490209769098"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f886e9f8e33a4c8791b47120af18b098"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f4d1c84a22e4ca6b3a8b5d648ef4d85"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = [\"summarize: \" + doc for doc in examples[\"article\"]]\n    model_inputs = tokenizer(\n        inputs,\n        max_length=512,\n        truncation=True,\n        padding=\"max_length\",\n    )\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[\"summary\"],\n            max_length=256,\n            truncation=True,\n            padding=\"max_length\",\n        )\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:57:20.552621Z","iopub.execute_input":"2025-06-02T08:57:20.552894Z","iopub.status.idle":"2025-06-02T08:57:20.558210Z","shell.execute_reply.started":"2025-06-02T08:57:20.552874Z","shell.execute_reply":"2025-06-02T08:57:20.557471Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"tokenized_datasets = split_dataset.map(preprocess_function, batched=True)\ntokenized_datasets = tokenized_datasets.remove_columns([\"article\", \"summary\"])\nprint(\"Tokenized datasets:\", tokenized_datasets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:57:20.559194Z","iopub.execute_input":"2025-06-02T08:57:20.559490Z","iopub.status.idle":"2025-06-02T08:57:26.620254Z","shell.execute_reply.started":"2025-06-02T08:57:20.559464Z","shell.execute_reply":"2025-06-02T08:57:26.619419Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4003 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5518f9c7fe1e4ebab08df48e28937b0a"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/445 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6fae5922cd0498d96cae32e9dbdfc64"}},"metadata":{}},{"name":"stdout","text":"Tokenized datasets: DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 4003\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 445\n    })\n})\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"train_dataset = tokenized_datasets[\"train\"].select(range(4000))\neval_dataset = tokenized_datasets[\"test\"].select(range(410)) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:57:26.621361Z","iopub.execute_input":"2025-06-02T08:57:26.621661Z","iopub.status.idle":"2025-06-02T08:57:26.630387Z","shell.execute_reply.started":"2025-06-02T08:57:26.621634Z","shell.execute_reply":"2025-06-02T08:57:26.629687Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"print(\"Train dataset:\", train_dataset) \nprint(\"Validation dataset:\", eval_dataset) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:57:26.632970Z","iopub.execute_input":"2025-06-02T08:57:26.633216Z","iopub.status.idle":"2025-06-02T08:57:26.647595Z","shell.execute_reply.started":"2025-06-02T08:57:26.633198Z","shell.execute_reply":"2025-06-02T08:57:26.646907Z"}},"outputs":[{"name":"stdout","text":"Train dataset: Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 4000\n})\nValidation dataset: Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 410\n})\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"lora_config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM,\n    r=8,  \n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"q\", \"v\"],  \n)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:57:26.659343Z","iopub.execute_input":"2025-06-02T08:57:26.659540Z","iopub.status.idle":"2025-06-02T08:57:26.756125Z","shell.execute_reply.started":"2025-06-02T08:57:26.659525Z","shell.execute_reply":"2025-06-02T08:57:26.755304Z"}},"outputs":[{"name":"stdout","text":"trainable params: 884,736 || all params: 223,788,288 || trainable%: 0.3953\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"/kaggle/working/lora-t5-news-summarization\",eval_strategy=\"epoch\",learning_rate=1e-3,per_device_train_batch_size=5,per_device_eval_batch_size=5,\n    num_train_epochs=3,weight_decay=0.01,\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    load_best_model_at_end=True,fp16=True, mak\n    logging_dir=\"/kaggle/working/logs\",logging_steps=100,\n    report_to='none'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:57:26.756916Z","iopub.execute_input":"2025-06-02T08:57:26.757147Z","iopub.status.idle":"2025-06-02T08:57:26.794100Z","shell.execute_reply.started":"2025-06-02T08:57:26.757122Z","shell.execute_reply":"2025-06-02T08:57:26.793508Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"rouge = evaluate.load(\"rouge\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    \n    # Replace -100 in the labels as we can't decode them\n    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    return {k: round(v, 4) for k, v in result.items()}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:57:26.794790Z","iopub.execute_input":"2025-06-02T08:57:26.795017Z","iopub.status.idle":"2025-06-02T09:20:45.917511Z","shell.execute_reply.started":"2025-06-02T08:57:26.795001Z","shell.execute_reply":"2025-06-02T09:20:45.916755Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1200/1200 23:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.456300</td>\n      <td>0.395445</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.461600</td>\n      <td>0.373204</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.431900</td>\n      <td>0.366364</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1200, training_loss=0.5471664460500082, metrics={'train_runtime': 1398.1551, 'train_samples_per_second': 8.583, 'train_steps_per_second': 0.858, 'total_flos': 7340109594624000.0, 'train_loss': 0.5471664460500082, 'epoch': 3.0})"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/lora-t5-news-final\")\ntokenizer.save_pretrained(\"/kaggle/working/lora-t5-news-final\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T09:20:45.918408Z","iopub.execute_input":"2025-06-02T09:20:45.918947Z","iopub.status.idle":"2025-06-02T09:20:46.045196Z","shell.execute_reply.started":"2025-06-02T09:20:45.918929Z","shell.execute_reply":"2025-06-02T09:20:46.044551Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/lora-t5-news-final/tokenizer_config.json',\n '/kaggle/working/lora-t5-news-final/special_tokens_map.json',\n '/kaggle/working/lora-t5-news-final/spiece.model',\n '/kaggle/working/lora-t5-news-final/added_tokens.json',\n '/kaggle/working/lora-t5-news-final/tokenizer.json')"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"metrics = trainer.evaluate()\nprint(\"Evaluation metrics:\", metrics)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T09:20:46.045889Z","iopub.execute_input":"2025-06-02T09:20:46.046209Z","iopub.status.idle":"2025-06-02T09:20:46.049822Z","shell.execute_reply.started":"2025-06-02T09:20:46.046184Z","shell.execute_reply":"2025-06-02T09:20:46.049060Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom peft import PeftModel, PeftConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T09:20:46.062830Z","iopub.execute_input":"2025-06-02T09:20:46.063110Z","iopub.status.idle":"2025-06-02T09:20:46.073773Z","shell.execute_reply.started":"2025-06-02T09:20:46.063088Z","shell.execute_reply":"2025-06-02T09:20:46.073205Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"model_path = \"/kaggle/working/lora-t5-news-final\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\npeft_config = PeftConfig.from_pretrained(model_path)\nbase_model = AutoModelForSeq2SeqLM.from_pretrained(peft_config.base_model_name_or_path)\nmodel = PeftModel.from_pretrained(base_model, model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T09:20:46.074473Z","iopub.execute_input":"2025-06-02T09:20:46.074655Z","iopub.status.idle":"2025-06-02T09:20:46.520744Z","shell.execute_reply.started":"2025-06-02T09:20:46.074642Z","shell.execute_reply":"2025-06-02T09:20:46.519914Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T09:20:46.521738Z","iopub.execute_input":"2025-06-02T09:20:46.522137Z","iopub.status.idle":"2025-06-02T09:20:46.935047Z","shell.execute_reply.started":"2025-06-02T09:20:46.522112Z","shell.execute_reply":"2025-06-02T09:20:46.934334Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"PeftModelForSeq2SeqLM(\n  (base_model): LoraModel(\n    (model): T5ForConditionalGeneration(\n      (shared): Embedding(32128, 768)\n      (encoder): T5Stack(\n        (embed_tokens): Embedding(32128, 768)\n        (block): ModuleList(\n          (0): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                  (relative_attention_bias): Embedding(32, 12)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseActDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1-11): 11 x T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseActDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): T5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (decoder): T5Stack(\n        (embed_tokens): Embedding(32128, 768)\n        (block): ModuleList(\n          (0): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                  (relative_attention_bias): Embedding(32, 12)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseActDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1-11): 11 x T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseActDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): T5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"text = \"\"\"\n Today, India commemorates its 77th Independence Day, marking the end of nearly two centuries of British colonial rule. On this day in 1947, India emerged as a free nation after a long and arduous struggle for independence led by figures such as Mahatma Gandhi, Jawaharlal Nehru, Sardar Vallabhbhai Patel, and countless others. As the national flag was hoisted above the Red Fort in New Delhi, Prime Minister Nehru’s famous \"Tryst with Destiny\" speech marked the beginning of a new era.\n\nIndia’s journey from colonial rule to becoming the world’s largest democracy has been nothing short of remarkable. The adoption of the Constitution in 1950 laid the foundation for a sovereign, socialist, secular, and democratic republic. Over the decades, India has made significant progress in sectors such as technology, education, and space exploration, while still striving to address challenges related to poverty, inequality, and regional conflicts.\n\nHistorical milestones such as the Green Revolution, the 1991 economic liberalization, and the launch of the Mars Orbiter Mission in 2013 have shaped modern India’s identity. Cultural heritage from ancient civilizations like the Indus Valley and contributions to philosophy, mathematics, and medicine continue to influence the global stage. India’s diverse culture and unity in diversity remain central to its national identity.\n\nAs the country celebrates another year of freedom, citizens are encouraged to reflect not just on past glories, but also on the responsibilities of shaping a just, inclusive, and sustainable future. Independence Day serves not only as a tribute to the sacrifices of the past but also as a call to action for future generations.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T09:20:46.935870Z","iopub.execute_input":"2025-06-02T09:20:46.936189Z","iopub.status.idle":"2025-06-02T09:20:46.940477Z","shell.execute_reply.started":"2025-06-02T09:20:46.936170Z","shell.execute_reply":"2025-06-02T09:20:46.939808Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"inputs = tokenizer(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n\noutputs = model.generate(**inputs, max_length=300, min_length=80 ,num_beams=4 ,early_stopping=True  ,length_penalty=1.0, no_repeat_ngram_size=3)\nsummary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(\"summary:\", summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T09:25:07.595790Z","iopub.execute_input":"2025-06-02T09:25:07.596590Z","iopub.status.idle":"2025-06-02T09:25:11.716092Z","shell.execute_reply.started":"2025-06-02T09:25:07.596556Z","shell.execute_reply":"2025-06-02T09:25:11.715297Z"}},"outputs":[{"name":"stdout","text":"summary: India’s diverse culture and unity in diversity remain central to its national identity.On this day in 1947, India emerged as a free nation after a long and arduous struggle for independence led by figures such as Mahatma Gandhi, Jawaharlal Nehru, Sardar Vallabhbhai Patel, and countless others.India’s journey from colonial rule to becoming the world’s largest democracy has been nothing short of remarkable.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import shutil\n\nmodel_dir = \"/kaggle/working/lora-t5-news-final\"\n\nshutil.make_archive(model_dir, 'zip', model_dir)\n\nprint(\"✅ Model zipped successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T09:20:51.466269Z","iopub.status.idle":"2025-06-02T09:20:51.466552Z","shell.execute_reply.started":"2025-06-02T09:20:51.466432Z","shell.execute_reply":"2025-06-02T09:20:51.466444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}